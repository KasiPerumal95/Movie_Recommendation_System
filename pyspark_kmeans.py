# -*- coding: utf-8 -*-
"""PySpark_KMeans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y5s5Iir6oOdfXZYz9JO4podtODN3tsFA
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://www-us.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
!tar xf spark-2.4.4-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

!pip install pyspark

from itertools import chain
import numpy as np
import pandas as pd

rating = pd.read_csv("ratings.dat.txt", sep = "::", header = None )

movies = pd.read_csv("movies.dat.txt", sep = "::", header = None )

df = pd.DataFrame(movies)
df.columns = ['MovieID','Title','Genres']

df.head()

def chainer(s):
    return list(chain.from_iterable(s.str.split('|')))

lens = df['Genres'].str.split('|').map(len)

result = pd.DataFrame({'MovieID': np.repeat(df['MovieID'], lens),
                    'Title': np.repeat(df['Title'], lens),
                    'Genres': chainer(df['Genres'])})

final_result = result

final_result["Values"] = 1

final_result.head()

pivot_table = pd.pivot_table(final_result, index = ["MovieID","Title"],columns = "Genres", values= 'Values')
pivot_table.fillna(0,inplace=True)

pivot_table.head()

pivot_table.to_csv("final_value_cluster_1",header= False, index=False)

pivot_table.to_csv("final_Pivot_cluster.csv")

"""K-Means Clustering"""

from pyspark.mllib.clustering import KMeans, KMeansModel
import pyspark
from pyspark import SparkContext

from numpy import array
from math import sqrt

from sklearn.metrics import mean_squared_error

sc = SparkContext()

sample = sc.textFile("final_value_cluster_1")



parsedData_2= sample.map(lambda line: array([float(x) for x in line.split(',')]))

def error(point):
    center = clusters_2.centers[clusters_2.predict(point)]
    return sqrt(sum([x**2 for x in (point - center)]))

for i in range(2,20):
  clusters_2 = KMeans.train(parsedData_2, i, maxIterations=100)
  WSSSE = parsedData_2.map(lambda point: error(point)).reduce(lambda x, y: x + y)
  print("Within Root Mean Squared Error for {} cluster= ".format(i) + str(WSSSE))



clusters_2 = KMeans.train(parsedData_2, 18, maxIterations=100)
WSSSE = parsedData_2.map(lambda point: error(point)).reduce(lambda x, y: x + y)
print("Within Root Mean Squared Error for 18 cluster= " + str(WSSSE))

labels = clusters_2.predict(parsedData_2)

labels_value = labels.collect()

max(labels_value)

len(labels_value)

data_set = pd.read_csv("final_Pivot_cluster.csv")

data_set['cluster_label'] = labels_value

data_set.head()

data_Movie_Cluster = data_set[['MovieID','cluster_label']]

data_Movie_Cluster.head()

data = {}

for i in range(len(data_Movie_Cluster)):
  data[data_Movie_Cluster.iloc[i,0]] = data_Movie_Cluster.iloc[i,1]

print(len(data))

print(data)

dataRating = pd.read_csv("ratings.dat.txt", sep="::",header=None)

dataRating.head()

dataRating.columns = ['UserID','MovieID','Ratings','Time']

data_frame = pd.merge(dataRating,data_Movie_Cluster, how = 'inner', on = 'MovieID' )

data_frame.sort_values('UserID',ascending= True, inplace= True)

data_frame.head()

dataFrame_2 = data_frame.groupby(['UserID','cluster_label'])['Ratings'].mean()

dataFrame_2.head()

Final_data_frame = pd.DataFrame(dataFrame_2)

Final_data_frame.head()

Pivot_data = pd.pivot_table(Final_data_frame,index='UserID', columns='cluster_label',values='Ratings')

Pivot_data.fillna(0,inplace= True)

Pivot_data.head()

Predicted_value = []

for i in range(len(data_frame)):
  Predicted_value.append(Pivot_data.iloc[data_frame.iloc[i,0]-1,data_frame.iloc[i,4]])

print(len(Predicted_value))

data_frame['Predicted_values'] = Predicted_value

data_frame.head()

RMSE = np.sqrt(mean_squared_error(data_frame['Ratings'], data_frame['Predicted_values']))

print("The Root Mean Squared Error rate is {}".format(RMSE))

"""ALS Code"""

import math

from pyspark.mllib.recommendation import ALS, Rating, MatrixFactorizationModel

rating_data = sc.textFile("ratings.dat.txt")

rating_dataset = rating_data.map(lambda x: x.split("::")).cache()

data_set_rating =rating_dataset.map(lambda x: Rating(int(x[0]),int(x[1]),float(x[2]))).cache()
print data_set_rating.take(3)

training_RDD, validation_RDD, test_RDD = data_set_rating.randomSplit([6, 2, 2], seed=0L)
validation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))
test_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))

seed = 5L
iterations = [10,20,30]
regularization_parameter = [0.1,0.01]
ranks = [2,4,6,8,10,12]
errors = []
err = 0
tolerance = 0.02

min_error = float('inf')
best_rank = -1
best_iteration = -1
best_regularization = -1
for rank in ranks:
  for iterat in iterations:
    for regular in regularization_parameter:
      
      model = ALS.train(training_RDD, rank, seed=seed, iterations=iterat,
                      lambda_=regular)
      predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))
      rates_and_preds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)
      error = math.sqrt(rates_and_preds.map(lambda r: (r[1][0] - r[1][1])**2).mean())
      errors.append(error)
      err += 1
      print('For rank %s, Iteration %s and Regular Parameter %s the RMSE is %s' % (rank,iterat, regular, error))
      if error < min_error:
        min_error = error
        best_rank = rank
        best_iteration = iterat
        best_regularization = regular


print('The best model was trained with rank %s, iterations %s, regularized paramater %s' % (best_rank, best_iteration,best_regularization))

